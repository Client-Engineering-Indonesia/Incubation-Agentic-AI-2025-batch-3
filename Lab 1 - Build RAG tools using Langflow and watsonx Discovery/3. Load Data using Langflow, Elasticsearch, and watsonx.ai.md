# Load Data using Langflow, Elasticsearch, and watsonx.ai

In this step, we will customize our Langflow *📚 Load Data Flow* with watsonx.ai and ElasticSearch

## 1. Add & Delete Components from Template

#### 1. In the *Components* section on the right of the Langflow screen, search for *"IBM"*
#### 2. Add the *"watsonx.ai Embedding"* component.
   - Delete the default **OpenAI Embedding** component.
   <img width="1440" height="900" alt="Screenshot 2025-07-17 at 17 24 31" src="https://github.com/user-attachments/assets/93366a5f-8476-4354-a27a-14013fb958a9" />

#### 3. Now search for *Elasticsearch*
   - Add the **Elasticsearch** component.
   - Delete the existing **AstraDB** component.
   <img width="1440" height="900" alt="Screenshot 2025-07-17 at 17 25 18" src="https://github.com/user-attachments/assets/cef9546a-3805-42fd-a6f9-f81f7c489db7" />

#### 4. Now search for *Parser* and add
   <img width="1440" height="900" alt="Screenshot 2025-07-17 at 17 27 05" src="https://github.com/user-attachments/assets/2804592d-e5dc-4eeb-91ee-fc9c45237aa2" />

## 2. Fill Component Details
Each component contains fields where you must enter your credentials or configuration settings.

#### 1. File
This is where you upload the input file that will be used to generate embeddings.
- **Download the file** [`workshop 24 july 2025 data dummy`](https://github.com/Incubation-Agentic-AI-2025-batch-2/Lab-1-Build-RAG-tools-using-Langflow-and-watsonx-Discovery/blob/main/assets/workshop%2024%20july%202025%20data%20dummy). This file is provided in the `main` branch, inside the `assets` folder of this repository.

- Upload this file into the **File** component.
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 54 29" src="https://github.com/user-attachments/assets/b577ce24-a58a-4686-83ad-34cd9214d54b" />


#### 2. Split Text
The **Split Text** component breaks up long documents into smaller text chunks based on rules (e.g., number of characters or sentences).
**Instructions:**
- Click the **Split Text** component.
- Click the **Controls** button.
  <img width="696" height="293" alt="Screenshot 2025-07-17 at 17 32 21" src="https://github.com/user-attachments/assets/6d444441-cd9f-4687-b90c-d0f661fed80d" />

- Fill in the details (chunk size, overlap, etc.) as shown in the provided screenshot or lab instructions.
  <img width="1440" height="900" alt="Screenshot 2025-07-17 at 17 33 25" src="https://github.com/user-attachments/assets/b929632a-0d68-41fe-99f0-a5ce94e4a0a5" />

  

#### 3. Parser
The **Parser** component formats your input text using a specified template. This is necessary to create consistent structure for each text chunk before sending it to the embedding model.
#### Modify the Code:

- Click on the **Parser** component.
- Click the **Code** button at the top.
  <img width="430" height="276" alt="Screenshot 2025-07-17 at 16 35 36" src="https://github.com/user-attachments/assets/0692e449-99dd-4fea-8125-430e611624b1" />

- Select all and **delete all** the code in the existing code editor
  <img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 48 23" src="https://github.com/user-attachments/assets/f5e195f2-d64b-41c3-85e3-430514d5d32c" />

- **Copy and paste the code** provided below into the code editor.
```
from langflow.custom.custom_component.component import Component
from langflow.helpers.data import safe_convert
from langflow.inputs.inputs import BoolInput, HandleInput, MessageTextInput, MultilineInput, TabInput
from langflow.schema.data import Data
from langflow.schema.dataframe import DataFrame
from langflow.template.field.base import Output


class ParserComponent(Component):
    display_name = "Parser"
    description = "Extracts text using a template."
    documentation: str = "https://docs.langflow.org/components-processing#parser"
    icon = "braces"

    inputs = [
        HandleInput(
            name="input_data",
            display_name="Data or DataFrame",
            input_types=["DataFrame", "Data"],
            info="Accepts either a DataFrame or a Data object.",
            required=True,
        ),
        TabInput(
            name="mode",
            display_name="Mode",
            options=["Parser", "Stringify"],
            value="Parser",
            info="Convert into raw string instead of using a template.",
            real_time_refresh=True,
        ),
        MultilineInput(
            name="pattern",
            display_name="Template",
            info=(
                "Use variables within curly brackets to extract column values for DataFrames "
                "or key values for Data. "
                "Example: `Text: {text}`"
            ),
            value="Text: {text}",
            dynamic=True,
            show=True,
            required=True,
        ),
        MessageTextInput(
            name="sep",
            display_name="Separator",
            advanced=True,
            value="\n",
            info="String used to separate rows/items.",
        ),
    ]

    outputs = [
        Output(
            display_name="Parsed Text",
            name="parsed_text",
            info="Formatted text output.",
            method="parse_combined_text",
            output_type="Data",
            is_list=True,
        ),
    ]

    def update_build_config(self, build_config, field_value, field_name=None):
        if field_name == "mode":
            build_config["pattern"]["show"] = self.mode == "Parser"
            build_config["pattern"]["required"] = self.mode == "Parser"
            if field_value:
                clean_data = BoolInput(
                    name="clean_data",
                    display_name="Clean Data",
                    info=(
                        "Enable to clean the data by removing empty rows and lines "
                        "in each cell of the DataFrame/ Data object."
                    ),
                    value=True,
                    advanced=True,
                    required=False,
                )
                build_config["clean_data"] = clean_data.to_dict()
            else:
                build_config.pop("clean_data", None)
        return build_config

    def _clean_args(self):
        input_data = self.input_data
        match input_data:
            case list() if all(isinstance(item, Data) for item in input_data):
                raise ValueError("List of Data objects is not supported.")
            case DataFrame():
                return input_data, None
            case Data():
                return None, input_data
            case dict() if "data" in input_data:
                try:
                    if "columns" in input_data:
                        return DataFrame.from_dict(input_data), None
                    return None, Data(**input_data)
                except (TypeError, ValueError, KeyError) as e:
                    raise ValueError(f"Invalid structured input provided: {e!s}") from e
            case _:
                raise ValueError(f"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.")

    def parse_combined_text(self) -> list[Data]:
        """Returns list of Data (one per chunk/row), each with only chunk_id in metadata."""
        if self.mode == "Stringify":
            return [self.convert_to_string()]

        df, data = self._clean_args()
        result = []

        if df is not None:
            for i, (_, row) in enumerate(df.iterrows()):
                row_dict = row.to_dict()
                formatted_text = self.pattern.format(**row_dict)
                result.append(
                    Data(
                        data={"text": formatted_text},
                        metadata={"chunk_id": i}
                    )
                )
        elif data is not None:
            formatted_text = self.pattern.format(**data.data)
            result.append(
                Data(
                    data={"text": formatted_text},
                    metadata={"chunk_id": 0}
                )
            )

        self.status = f"{len(result)} chunks parsed."
        return result

    def convert_to_string(self) -> Data:
        """Fallback if stringify mode is selected."""
        result = ""
        if isinstance(self.input_data, list):
            result = "\n".join([
                safe_convert(item, clean_data=self.clean_data or False)
                for item in self.input_data
            ])
        else:
            result = safe_convert(self.input_data or False)

        self.log(f"Converted to string with length: {len(result)}")

        return Data(
            data={"text": result},
            metadata={"chunk_id": 0}
        )
```
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 48 32" src="https://github.com/user-attachments/assets/3d9f5fba-5246-4f34-998a-76b66c8fe183" />

> [!NOTE]
> ### 💻 What does this custom Parser code do?
>
> The customized `ParserComponent` improves how your data is prepared before it gets embedded and stored in Elasticsearch.
>
> #### ✅ Key Functionalities:
> - **Transforms structured data (DataFrame or Data)** into clean, individual text chunks.
> - Each output is wrapped in a `Data` object with:
>   - `"text"`: the formatted content from your template
>   - `"chunk_id"`: a numeric ID for tracking or referencing
> - The output is a **list of Data objects**, not a single text blob — making it **compatible with chunk-based vector search**.
> #### 🧠 Why is this useful?
> - When building **Retrieval-Augmented Generation (RAG)** pipelines, you need to **split content into chunks**, so each chunk can be embedded, stored, and retrieved independently.
> - Adding metadata like `chunk_id` helps trace the chunk's origin during debugging or post-processing.
> - Structured output improves compatibility with components like Elasticsearch, Langchain retrievers, and downstream LLM workflows.
>
> In short, this code turns raw data into **search-ready, chunked, and well-formatted units** for your AI pipeline.

#### 4. IBM watsonx.ai Embeddings
This component is used to generate embeddings from your input text using IBM’s foundation models.
- Select API Endpoint (select **us-south-ml**)
- Enter watsonx Project ID & API Key: **from emailed document** (Locate the **"Project_ID"** & **"Apikey"** column to find your assigned watsonx.ai Project ID & API Key)

     <img width="548" height="33" alt="Screenshot 2025-07-23 at 19 53 09" src="https://github.com/user-attachments/assets/da1d1455-d8d0-4aec-ac04-366bd32d7602" />
- Select model name. You can explore available **IBM watsonx.ai** embedding models from the dropdown
  <img width="1440" height="812" alt="Screenshot 2025-07-22 at 13 50 24" src="https://github.com/user-attachments/assets/6ab62fab-3ac9-4ac5-828a-bc94a0a00ce5" />



#### 5. Elasticsearch

This component stores the embeddings so they can be retrieved later during question-answering.
- Fill in
   - URL, Username, Password  : **from emailed document**, in page **Langflow&Elasticsearch accounts**
     - Copy and paste the **"Elasticsearch URL", "Elasticsearch Username", and "Elasticsearch Password** according to your assigned account number.

<img width="1375" height="20" alt="Screenshot 2025-07-23 at 19 56 46" src="https://github.com/user-attachments/assets/3ca55621-d9e3-408a-bef5-ccf7247d2d5c" />

   - Index Name: use the index name you created before
- Disable "Verify SSL Certificates"
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 53 35" src="https://github.com/user-attachments/assets/33c0a665-51ca-472e-bbc6-972f346875ca" />
<img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 53 48" src="https://github.com/user-attachments/assets/54576240-41f2-4bd6-b79e-88f6de4f7372" />


## 3. Run *📚 Load Data Flow*
1. Ensure all components are connected: File → Split → Parser → Embedding → Elasticsearch
   <img width="893" height="520" alt="Screenshot 2025-07-18 at 11 42 05" src="https://github.com/user-attachments/assets/e096ca20-7faa-4394-b567-685f164bf055" />
 
2. On the Elasticsearch component, click the ▶️ button to run the flow
   <img width="652" height="409" alt="Screenshot 2025-07-18 at 11 37 24" src="https://github.com/user-attachments/assets/82c97b09-ac12-4107-9b6c-06ba53ab60c1" />
 
3. Click the 🔍 icon to view stored data  
   - You can inspect results at any component along the flow
     
      <img width="321" height="643" alt="Screenshot 2025-07-18 at 11 37 44" src="https://github.com/user-attachments/assets/c6be6f34-84a6-49ad-8be9-b44861fe57e5" />
     <img width="1440" height="900" alt="Screenshot 2025-07-17 at 11 56 51" src="https://github.com/user-attachments/assets/5ae970e1-ea37-4931-a076-5906a6115bb4" />
